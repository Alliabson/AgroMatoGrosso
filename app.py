import streamlit as st
from streamlit_folium import st_folium
import pandas as pd
import folium
import requests
from bs4 import BeautifulSoup
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
import time
import re

# ==============================================================================
# FUN√á√ïES AUXILIARES
# ==============================================================================

def is_pessoa_juridica(nome):
    """
    Verifica se um nome provavelmente pertence a uma empresa.
    Esta √© a nossa fun√ß√£o "funil".
    """
    if not nome or pd.isna(nome):
        return False
        
    # Lista de palavras-chave que indicam ser uma empresa
    keywords = ['ltda', 's.a', 's/a', 'agropecu√°ria', 'agr√≠cola', 
                'fazenda', 'grupo', 'agro', 'produtos', 'investimentos',
                'com√©rcio', 'agricola', 'algod√£o', 'algodao', 'cotton',
                'industrial', 'exporta√ß√£o', 'exportadora', 'comercial']
    
    nome_lower = nome.lower()
    for keyword in keywords:
        if keyword in nome_lower:
            return True
    return False

def extrair_cidade_do_nome(nome):
    """
    Tenta extrair a cidade do nome da empresa quando dispon√≠vel
    """
    padroes_cidade = [
        r'\- ([A-Za-z\s]+)$',  # Padr√£o "Empresa - Cidade"
        r'\- ([A-Za-z\s]+) \-', # Padr√£o "Empresa - Cidade -"
        r'([A-Za-z\s]+) \- MT$' # Padr√£o "Cidade - MT"
    ]
    
    for padrao in padroes_cidade:
        match = re.search(padrao, nome)
        if match:
            return match.group(1).strip()
    
    return "N√£o Informada"

# ==============================================================================
# FUN√á√ÉO MESTRA PARA COLETAR E PROCESSAR OS DADOS
# ==============================================================================

@st.cache_data(show_spinner=False)
def carregar_e_processar_dados():
    """
    Fun√ß√£o unificada que executa o web scraping de tabelas (com pagina√ß√£o) 
    e a geocodifica√ß√£o, filtrando apenas por pessoas jur√≠dicas.
    """
    # --- Parte 1: Web Scraping de Tabela com Pagina√ß√£o ---
    st.write("üåê Iniciando coleta de dados via web scraping...")
    
    url_base = "https://ampa.com.br/consulta-associados-ativos/"
    lista_empresas = []
    pagina_num = 1
    max_paginas = 20  # Limite de seguran√ßa

    while pagina_num <= max_paginas:
        st.write(f"üìÑ Coletando dados da p√°gina {pagina_num}...")
        
        # Constr√≥i a URL da p√°gina atual
        if pagina_num == 1:
            url_atual = url_base
        else:
            url_atual = f"{url_base}page/{pagina_num}/"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url_atual, headers=headers, timeout=30)
            response.raise_for_status()
            
            # Verifica se a p√°gina existe
            if response.status_code == 404 and pagina_num > 1:
                st.write("‚úÖ Todas as p√°ginas foram coletadas.")
                break
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Procura por tabelas - m√©todo mais flex√≠vel
            tabelas = soup.find_all('table')
            
            if not tabelas:
                if pagina_num == 1:
                    st.error("‚ùå N√£o foi poss√≠vel encontrar tabelas na p√°gina. A estrutura do site pode ter mudado.")
                    st.info("üí° Dica: Verifique se o site https://ampa.com.br/consulta-associados-ativos/ est√° acess√≠vel e cont√©m a lista de associados.")
                break

            empresas_encontradas_pagina = 0
            
            for tabela in tabelas:
                # Itera sobre todas as linhas da tabela
                for linha in tabela.find_all('tr'):
                    celulas = linha.find_all(['td', 'th'])
                    if len(celulas) >= 2:  # Pelo menos 2 colunas
                        nome = celulas[0].get_text(strip=True)
                        
                        # Pula cabe√ßalhos e linhas vazias
                        if not nome or nome.lower() in ['nome', 'empresa', 'associado']:
                            continue
                            
                        telefone = celulas[1].get_text(strip=True) if len(celulas) > 1 else "N√£o Informado"
                        
                        # APLICA O FUNIL: Processa apenas se for uma pessoa jur√≠dica
                        if is_pessoa_juridica(nome):
                            cidade = extrair_cidade_do_nome(nome)
                            
                            lista_empresas.append({
                                'Nome': nome,
                                'Telefone': telefone,
                                'Tipo': 'Algodoeira',
                                'Cidade': cidade,
                                'Estado': 'MT'
                            })
                            empresas_encontradas_pagina += 1

            st.write(f"‚úÖ {empresas_encontradas_pagina} empresas encontradas na p√°gina {pagina_num}")
            
            # Verifica se h√° pr√≥xima p√°gina
            next_button = soup.find('a', class_='next')
            if not next_button:
                # Alternativa: procura por numera√ß√£o de p√°gina
                pagination = soup.find('div', class_=['pagination', 'nav-links'])
                if not pagination:
                    st.write("‚úÖ √öltima p√°gina alcan√ßada.")
                    break
            
            pagina_num += 1
            time.sleep(2)  # Respeitoso com o servidor

        except requests.exceptions.RequestException as e:
            st.error(f"‚ùå Erro de conex√£o na p√°gina {pagina_num}: {e}")
            break
        except Exception as e:
            st.error(f"‚ùå Erro inesperado na p√°gina {pagina_num}: {e}")
            break

    if not lista_empresas:
        st.error("‚ùå Nenhuma empresa (Pessoa Jur√≠dica) foi coletada.")
        st.info("""
        **Poss√≠veis solu√ß√µes:**
        1. Verifique se o site https://ampa.com.br/consulta-associados-ativos/ est√° online
        2. A estrutura do site pode ter mudado
        3. Tente ajustar as palavras-chave na fun√ß√£o `is_pessoa_juridica`
        """)
        return pd.DataFrame()
        
    df = pd.DataFrame(lista_empresas)
    st.success(f"üìä Coleta conclu√≠da: {len(df)} empresas (PJs) encontradas em {pagina_num-1} p√°ginas.")

    # --- Parte 2: Geocodifica√ß√£o ---
    if len(df) > 0:
        st.write("üó∫Ô∏è Iniciando geocodifica√ß√£o. Este processo pode ser lento...")
        
        # Filtra empresas √∫nicas para evitar geocodifica√ß√£o duplicada
        df_unique = df.drop_duplicates(subset=['Nome']).copy()
        
        geolocator = Nominatim(user_agent="algodoeiras_mt_app")
        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.2)

        latitudes = []
        longitudes = []
        cidades_corrigidas = []
        enderecos_completos = []
        
        progress_bar = st.progress(0)
        total_empresas = len(df_unique)
        status_text = st.empty()

        for index, row in df_unique.iterrows():
            status_text.text(f"üìç Geocodificando: {row['Nome'][:50]}...")
            
            location = None
            query_attempts = [
                f"{row['Nome']}, Mato Grosso, Brasil",
                f"{row['Cidade']}, Mato Grosso, Brasil",
                "Cuiab√°, Mato Grosso, Brasil"  # Fallback para capital
            ]
            
            for query in query_attempts:
                try:
                    location = geocode(query)
                    if location:
                        break
                except Exception as e:
                    continue

            if location:
                latitudes.append(location.latitude)
                longitudes.append(location.longitude)
                enderecos_completos.append(location.address)
                
                # Tenta extrair a cidade do resultado
                address = location.raw.get('address', {})
                cidade = (address.get('city') or 
                         address.get('town') or 
                         address.get('village') or 
                         address.get('municipality') or 
                         row['Cidade'])
                cidades_corrigidas.append(cidade)
            else:
                latitudes.append(None)
                longitudes.append(None)
                enderecos_completos.append("N√£o Localizado")
                cidades_corrigidas.append(row['Cidade'])
            
            progress_bar.progress((index + 1) / total_empresas)

        df_unique['Latitude'] = latitudes
        df_unique['Longitude'] = longitudes
        df_unique['Cidade_Geocodificada'] = cidades_corrigidas
        df_unique['Endereco'] = enderecos_completos
        
        # Remove empresas que n√£o foram geocodificadas
        df_final = df_unique.dropna(subset=['Latitude', 'Longitude']).copy()
        
        status_text.text("‚úÖ Geocodifica√ß√£o finalizada!")
        st.success(f"üó∫Ô∏è {len(df_final)} empresas foram geocodificadas com sucesso.")
        
        return df_final
    else:
        return pd.DataFrame()

# ==============================================================================
# INTERFACE DO APLICATIVO STREAMLIT
# ==============================================================================

st.set_page_config(
    page_title="Mapa das Algodoeiras de MT", 
    layout="wide",
    page_icon="üå±"
)

st.title("üå± Mapa das Algodoeiras de Mato Grosso")
st.markdown("""
**Fonte dos dados:** [AMPA - Associa√ß√£o Mato-grossense dos Produtores de Algod√£o](https://ampa.com.br/consulta-associados-ativos/)

Os dados s√£o coletados e processados em tempo real atrav√©s de web scraping.
""")

# Barra lateral com informa√ß√µes
st.sidebar.header("‚ÑπÔ∏è Sobre o App")
st.sidebar.info("""
Este aplicativo coleta dados de algodoeiras associadas √† AMPA e as exibe em um mapa interativo.

**Funcionalidades:**
- Web scraping autom√°tico do site da AMPA
- Filtro inteligente para pessoas jur√≠dicas
- Geocodifica√ß√£o para obter coordenadas
- Visualiza√ß√£o em mapa interativo
""")

st.sidebar.header("üéõÔ∏è Filtros")

# Carregamento dos dados
with st.spinner('üîÑ Por favor, aguarde... Coletando e processando dados...'):
    df_empresas = carregar_e_processar_dados()

if df_empresas.empty:
    st.error("N√£o foi poss√≠vel carregar os dados. Tente recarregar a p√°gina ou verificar a conex√£o com o site da AMPA.")
else:
    st.success(f"‚úÖ Processo conclu√≠do! **{len(df_empresas)}** algodoeiras foram localizadas e mapeadas.")
    
    # Filtros
    cidades_disponiveis = ["Exibir Todas"] + sorted(df_empresas['Cidade_Geocodificada'].unique().tolist())
    cidade_selecionada = st.sidebar.selectbox("Filtrar por Cidade:", cidades_disponiveis)

    if cidade_selecionada == "Exibir Todas":
        df_filtrado = df_empresas
    else:
        df_filtrado = df_empresas[df_empresas['Cidade_Geocodificada'] == cidade_selecionada]

    # Estat√≠sticas
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total de Empresas", len(df_filtrado))
    with col2:
        st.metric("Cidades Encontradas", df_filtrado['Cidade_Geocodificada'].nunique())
    with col3:
        st.metric("Tipo", "Algodoeiras")

    if df_filtrado.empty:
        st.warning("Nenhuma empresa encontrada para o filtro selecionado.")
    else:
        # Tabela de dados
        st.subheader("üìã Lista de Algodoeiras")
        st.dataframe(
            df_filtrado[['Nome', 'Telefone', 'Cidade_Geocodificada', 'Estado']].reset_index(drop=True),
            use_container_width=True
        )
        
        # Mapa
        st.subheader("üó∫Ô∏è Mapa de Localiza√ß√µes")
        
        # Centraliza o mapa em Mato Grosso
        map_center = [-12.6819, -56.9211]  # Coordenadas aproximadas de MT
        if len(df_filtrado) > 0:
            map_center = [df_filtrado['Latitude'].mean(), df_filtrado['Longitude'].mean()]
        
        mapa = folium.Map(location=map_center, zoom_start=7)

        # Adiciona marcadores
        for index, empresa in df_filtrado.iterrows():
            popup_html = f"""
            <div style="min-width: 250px">
                <h4>{empresa['Nome']}</h4>
                <hr>
                <b>üìç Cidade:</b> {empresa['Cidade_Geocodificada']}<br>
                <b>üìû Telefone:</b> {empresa['Telefone']}<br>
                <b>üè¢ Tipo:</b> {empresa['Tipo']}<br>
                <b>üéØ Endere√ßo:</b> {empresa.get('Endereco', 'N√£o dispon√≠vel')}
            </div>
            """
            
            folium.Marker(
                location=[empresa['Latitude'], empresa['Longitude']],
                popup=folium.Popup(popup_html, max_width=300),
                tooltip=empresa['Nome'],
                icon=folium.Icon(color='green', icon='leaf', prefix='fa')
            ).add_to(mapa)
        
        # Exibe o mapa
        st_folium(mapa, width='100%', height=600, returned_objects=[])
        
        # Bot√£o de download
        st.download_button(
            label="üì• Baixar Dados em CSV",
            data=df_filtrado.to_csv(index=False, encoding='utf-8-sig'),
            file_name="algodoeiras_mato_grosso.csv",
            mime="text/csv"
        )
